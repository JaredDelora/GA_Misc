{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first import libraries\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier, BaggingRegressor, BaggingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score\n",
    "from warnings import simplefilter\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "simplefilter(\"ignore\", category=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_explorer:\n",
    "    # This object takes in a DataFrame and then does some initial exploration\n",
    "    def __init__(self, path):\n",
    "        # This initalizes and loads all our past datasets\n",
    "        self.dframe = pd.read_csv(path)\n",
    "        self.dtypes = self.dframe.dtypes\n",
    "        self.shape = self.dframe.shape\n",
    "        self.nulls = self.dframe.isnull().mean()\n",
    "        \n",
    "    def explore(self):\n",
    "        \n",
    "        # Let's check out the shape of our DataFrame    \n",
    "        print(f'DataFrame has {self.dframe.shape[1]} columns and {self.dframe.shape[0]} rows.')   \n",
    "        \n",
    "        # First we'll look for null values and Data Types\n",
    "        i = 1\n",
    "        for item1, item2 in zip(self.nulls, self.dtypes):\n",
    "            p = 0\n",
    "\n",
    "            if item1 != 0:\n",
    "                print(f'Column {i} has {item1} null items')\n",
    "                p += 1\n",
    "            print(f'Column {i} is {item2}')\n",
    "            i += 1   \n",
    "        if p == 0:\n",
    "            print(f'DataFrame has zero null values.')\n",
    "            \n",
    "    \n",
    "    # Drop selected columns\n",
    "    def col_drop(self, drop_list, inplace = True):\n",
    "        self.dframe.drop(columns=drop_list, inplace=True)\n",
    "    \n",
    "    # Check out why an object column isn't numeric\n",
    "    def num_check(self, column_name):\n",
    "        print(self.dframe[self.dframe[column_name].str.isnumeric() == False])\n",
    "     \n",
    "    # Displays a column is Descending Order\n",
    "    def view_asc(self, column_name, asc = False):\n",
    "        temp = self.dframe[column_name].sort_values(ascending = asc)\n",
    "        return temp\n",
    "    \n",
    "    # This will set any column values you choose in your selected column to NAN\n",
    "    def col_nan(self, column_name, s):\n",
    "        self.dframe[column_name].apply(lambda x: np.nan if x == s else int(x))\n",
    "    \n",
    "    # This will show us what the shape of the DFrame will be if we drop all NAN values\n",
    "    def s_if_drop(self):\n",
    "        self.dframe.dropna().shape\n",
    "    \n",
    "    # This method will fill any NAN value in the DataFrame with 0\n",
    "    def fill_na(self):\n",
    "        self.dframe.fillna(value = 0)\n",
    "    \n",
    "    # This will build a new DataFrame with just the columns we desire\n",
    "    def df_builder(self, column_list):\n",
    "        new_df = pd.DataFrame()\n",
    "        for item in column_list:\n",
    "            new_df[item] = self.dframe[item]\n",
    "        return new_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = data_explorer('401ksubs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107    199.041\n",
       "531     192.990\n",
       "8361    191.715\n",
       "1455    180.858\n",
       "1354    179.373\n",
       "         ...   \n",
       "2166     10.044\n",
       "2220     10.035\n",
       "3392     10.032\n",
       "6600     10.008\n",
       "4621     10.008\n",
       "Name: inc, Length: 9275, dtype: float64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_o.view_asc('inc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 11 columns and 9275 rows.\n",
      "Column 1 is int64\n",
      "Column 2 is float64\n",
      "Column 3 is int64\n",
      "Column 4 is int64\n",
      "Column 5 is int64\n",
      "Column 6 is int64\n",
      "Column 7 is float64\n",
      "Column 8 is int64\n",
      "Column 9 is int64\n",
      "Column 10 is float64\n",
      "Column 11 is int64\n",
      "DataFrame has zero null values.\n"
     ]
    }
   ],
   "source": [
    "# All datatypes are as expected and there are zero null values in the DataFrame\n",
    "df_o.explore()\n",
    "df = df_o.dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Dictionary  \n",
    "Contains data from 401ksubs.dta  \n",
    "  obs:  9,275                            \n",
    " vars:  11  \n",
    " 4 Sep 2001 13:50  \n",
    "table {align:left;display:block}\n",
    "-------------------------------------------------------------------------------  \n",
    "|variable name|type|variable label|\n",
    "|:--|---|---|\n",
    "e401k|byte|=1 if eligble for 401(k)|\n",
    "inc|float|inc^2|\n",
    "marr|byte|=1 if married|\n",
    "male|byte|=1 if male respondent|\n",
    "age|byte|age^2|\n",
    "fsize|byte|family size|\n",
    "nettfa|float|net total fin. assets, $1000|\n",
    "p401k|byte|=1 if participate in 401(k)|\n",
    "pira|byte| =1 if have IRA|\n",
    "incsq|float|inc^2|\n",
    "agesq|int| age^2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:*** Location would be valuable. Also employment status would be good to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:*** This would be racist as it's judging an applicant, not on their financial record, but on their race. It's assuming one race is more likely to act badly than another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:*** I don't think we should use the 'male' column for the same reasons I wouldn't want to include race. It's inherantly sexist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:*** Income Squared and Age Squared. These are both important variables and including them would allow our model to take these factors into greater account. But since we are trying to predict income, we shouldn't use Income Squared as a variable. This would cause data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:*** The descriptios for age and inc are wrong, they are listed as being squared. They not squared and so should not be listed in this manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Friday morning of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression\n",
    "- Ridge\n",
    "- LASSO\n",
    "- KNN\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class analyzer:\n",
    "    def __init__(self, df, features, target):\n",
    "        self.df = df\n",
    "        self.X = self.df[features]\n",
    "        self.y = self.df[target]\n",
    "             \n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        # Scale our data.\n",
    "        # Relabeling scaled data as \"Z\" is common.\n",
    "        sc = StandardScaler()\n",
    "        self.Z_train = sc.fit_transform(self.X_train)\n",
    "        self.Z_test = sc.transform(self.X_test)\n",
    "        \n",
    "    def go_reg(self):\n",
    "        # Instantiate models\n",
    "        lr = LinearRegression()\n",
    "        knn = KNeighborsRegressor()\n",
    "        dt = DecisionTreeRegressor()\n",
    "        bag = BaggingRegressor()\n",
    "        rf = RandomForestRegressor()\n",
    "        ada = AdaBoostRegressor()\n",
    "        sv = SVR()\n",
    "        \n",
    "        lr.fit(self.Z_train, self.y_train)\n",
    "        knn.fit(self.Z_train, self.y_train)\n",
    "        dt.fit(self.Z_train, self.y_train)\n",
    "        bag.fit(self.Z_train, self.y_train)\n",
    "        rf.fit(self.Z_train, self.y_train)\n",
    "        ada.fit(self.Z_train, self.y_train)\n",
    "        sv.fit(self.Z_train, self.y_train)\n",
    "        \n",
    "        print('---------------------------------')\n",
    "        print(f'LR Train Score {round(lr.score(self.Z_train, self.y_train), 2)}')\n",
    "        print(f'LR Test Score {round(lr.score(self.Z_test, self.y_test), 2)}')\n",
    "        print(' ')\n",
    "        print(f'KNN Train Score {round(knn.score(self.Z_train, self.y_train), 2)}')\n",
    "        print(f'KNN Test Score {round(knn.score(self.Z_test, self.y_test), 2)}')\n",
    "        print(' ')\n",
    "        print(f'DT Train Score {round(dt.score(self.Z_train, self.y_train), 2)}')\n",
    "        print(f'DT Test Score {round(dt.score(self.Z_test, self.y_test), 2)}')\n",
    "        print(' ')\n",
    "        print(f'Bag Train Score {round(bag.score(self.Z_train, self.y_train), 2)}')\n",
    "        print(f'Bag Test Score {round(bag.score(self.Z_test, self.y_test), 2)}')\n",
    "        print(' ')\n",
    "        print(f'RF Train Score {round(rf.score(self.Z_train, self.y_train), 2)}')\n",
    "        print(f'RF Test Score {round(rf.score(self.Z_test, self.y_test), 2)}')\n",
    "        print(' ')\n",
    "        print(f'ADA Train Score {round(ada.score(self.Z_train, self.y_train), 2)}')\n",
    "        print(f'ADA Test Score {round(ada.score(self.Z_test, self.y_test), 2)}')\n",
    "        print(' ')\n",
    "        print(f'SV Train Score {round(sv.score(self.Z_train, self.y_train), 2)}')\n",
    "        print(f'SV Test Score {round(sv.score(self.Z_test, self.y_test), 2)}')\n",
    "        \n",
    "    def go_class(self):\n",
    "        lg = LogisticRegression()\n",
    "        knn = KNeighborsClassifier()\n",
    "        dt = DecisionTreeClassifier()\n",
    "        bag = BaggingClassifier()\n",
    "        rf = RandomForestClassifier()\n",
    "        ada = AdaBoostClassifier()\n",
    "        sv = SVC()\n",
    "        \n",
    "        lg.fit(self.Z_train, self.y_train)\n",
    "        knn.fit(self.Z_train, self.y_train)\n",
    "        dt.fit(self.Z_train, self.y_train)\n",
    "        bag.fit(self.Z_train, self.y_train)\n",
    "        rf.fit(self.Z_train, self.y_train)\n",
    "        ada.fit(self.Z_train, self.y_train)\n",
    "        sv.fit(self.Z_train, self.y_train)        \n",
    "        \n",
    "        print(' ')\n",
    "        print('-----------------------------')\n",
    "        # Linear Regression\n",
    "        lg_pred_train = lg.predict(self.Z_train)\n",
    "        lg_pred_test = lg.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'LG F1 Train Score is {f1_score(self.y_train, lg_pred_train)}')\n",
    "        print(f'LG F1 Test Score is {f1_score(self.y_test, lg_pred_test)}')\n",
    "        \n",
    "        # KNN\n",
    "        knn_pred_train = knn.predict(self.Z_train)\n",
    "        knn_pred_test = knn.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'KNN F1 Train Score is {f1_score(self.y_train, knn_pred_train)}')\n",
    "        print(f'KNN F1 Test Score is {f1_score(self.y_test, knn_pred_test)}')\n",
    "\n",
    "        # Decision Tree\n",
    "        dt_pred_train = dt.predict(self.Z_train)\n",
    "        dt_pred_test = dt.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'DT F1 Train Score is {f1_score(self.y_train, dt_pred_train)}')\n",
    "        print(f'DT F1 Test Score is {f1_score(self.y_test, dt_pred_test)}')\n",
    "        \n",
    "        # Bagged\n",
    "        bag_pred_train = bag.predict(self.Z_train)\n",
    "        bag_pred_test = bag.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'Bagged F1 Train Score is {f1_score(self.y_train, bag_pred_train)}')\n",
    "        print(f'Bagged F1 Test Score is {f1_score(self.y_test, bag_pred_test)}')\n",
    "\n",
    "        # Random Forest\n",
    "        rf_pred_train = rf.predict(self.Z_train)\n",
    "        rf_pred_test = rf.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'RF F1 Train Score is {f1_score(self.y_train, rf_pred_train)}')\n",
    "        print(f'RF F1 Test Score is {f1_score(self.y_test, rf_pred_test)}')\n",
    "\n",
    "        # Adaboost\n",
    "        ada_pred_train = ada.predict(self.Z_train)\n",
    "        ada_pred_test = ada.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'Adaboost F1 Train Score is {f1_score(self.y_train, ada_pred_train)}')\n",
    "        print(f'Adaboost F1 Test Score is {f1_score(self.y_test, ada_pred_test)}')\n",
    "\n",
    "        # Support Vector Classifier\n",
    "        sv_pred_train = sv.predict(self.Z_train)\n",
    "        sv_pred_test = sv.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'SV F1 Train Score is {f1_score(self.y_train, sv_pred_train)}')\n",
    "        print(f'SV F1 Test Score is {f1_score(self.y_test, sv_pred_test)}')\n",
    "        print('--------------------------------')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def RMSE_Calc(self):\n",
    "        \n",
    "        lr = LinearRegression()\n",
    "        knn = KNeighborsRegressor()\n",
    "        dt = DecisionTreeRegressor()\n",
    "        bag = BaggingRegressor()\n",
    "        rf = RandomForestRegressor()\n",
    "        ada = AdaBoostRegressor()\n",
    "        sv = SVR()\n",
    "        \n",
    "        lr.fit(self.Z_train, self.y_train)\n",
    "        knn.fit(self.Z_train, self.y_train)\n",
    "        dt.fit(self.Z_train, self.y_train)\n",
    "        bag.fit(self.Z_train, self.y_train)\n",
    "        rf.fit(self.Z_train, self.y_train)\n",
    "        ada.fit(self.Z_train, self.y_train)\n",
    "        sv.fit(self.Z_train, self.y_train)\n",
    "        print(' ')\n",
    "        print('-----------------------------')\n",
    "        # Linear Regression\n",
    "        lr_pred_train = lr.predict(self.Z_train)\n",
    "        lr_pred_test = lr.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'LR RMSE Train Score is {mean_squared_error(self.y_train, lr_pred_train)**(0.5)}')\n",
    "        print(f'LR RMSE Test Score is {mean_squared_error(self.y_test, lr_pred_test)**(0.5)}')\n",
    "\n",
    "        # KNN\n",
    "        knn_pred_train = knn.predict(self.Z_train)\n",
    "        knn_pred_test = knn.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'KNN RMSE Train Score is {mean_squared_error(self.y_train, knn_pred_train)**(0.5)}')\n",
    "        print(f'KNN RMSE Test Score is {mean_squared_error(self.y_test, knn_pred_test)**(0.5)}')\n",
    "\n",
    "        # Decision Tree\n",
    "        dt_pred_train = dt.predict(self.Z_train)\n",
    "        dt_pred_test = dt.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'DT RMSE Train Score is {mean_squared_error(self.y_train, dt_pred_train)**(0.5)}')\n",
    "        print(f'DT RMSE Test Score is {mean_squared_error(self.y_test, dt_pred_test)**(0.5)}')\n",
    "        \n",
    "        # Bagged\n",
    "        bag_pred_train = bag.predict(self.Z_train)\n",
    "        bag_pred_test = bag.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'Bagged RMSE Train Score is {mean_squared_error(self.y_train, bag_pred_train)**(0.5)}')\n",
    "        print(f'Bagged RMSE Test Score is {mean_squared_error(self.y_test, bag_pred_test)**(0.5)}')\n",
    "\n",
    "        # Random Forest\n",
    "        rf_pred_train = rf.predict(self.Z_train)\n",
    "        rf_pred_test = rf.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'RF RMSE Train Score is {mean_squared_error(self.y_train, rf_pred_train)**(0.5)}')\n",
    "        print(f'RF RMSE Test Score is {mean_squared_error(self.y_test, rf_pred_test)**(0.5)}')\n",
    "\n",
    "        # Adaboost\n",
    "        ada_pred_train = ada.predict(self.Z_train)\n",
    "        ada_pred_test = ada.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'Adaboost RMSE Train Score is {mean_squared_error(self.y_train, ada_pred_train)**(0.5)}')\n",
    "        print(f'Adaboost RMSE Test Score is {mean_squared_error(self.y_test, ada_pred_test)**(0.5)}')\n",
    "\n",
    "        # Support Vector Classifier\n",
    "        sv_pred_train = sv.predict(self.Z_train)\n",
    "        sv_pred_test = sv.predict(self.Z_test)\n",
    "        print(' ')\n",
    "        print(f'SVC RMSE Train Score is {mean_squared_error(self.y_train, sv_pred_train)**(0.5)}')\n",
    "        print(f'SVC RMSE Test Score is {mean_squared_error(self.y_test, sv_pred_test)**(0.5)}')\n",
    "        print('--------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['male', 'age', 'fsize', 'nettfa', 'marr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = analyzer(df, features, 'inc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LR Train Score 0.28\n",
      "LR Test Score 0.23\n",
      " \n",
      "KNN Train Score 0.52\n",
      "KNN Test Score 0.31\n",
      " \n",
      "DT Train Score 0.99\n",
      "DT Test Score -0.26\n",
      " \n",
      "Bag Train Score 0.86\n",
      "Bag Test Score 0.27\n",
      " \n",
      "RF Train Score 0.9\n",
      "RF Test Score 0.31\n",
      " \n",
      "ADA Train Score 0.29\n",
      "ADA Test Score 0.26\n",
      " \n",
      "SV Train Score 0.31\n",
      "SV Test Score 0.31\n"
     ]
    }
   ],
   "source": [
    "# The model is horrible. Large bias, little variance. More complexity needs to be added.\n",
    "obj.go_reg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Random Resampling with Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Decision Trees models tend to be overfit. Bagging solves this problem by using different trees based on subsamples of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** These two methods only differ in one way. Random forests use a modified algorithm that randomly chooses a subset of features at each branch split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Bagged Decision Tress tend to overfit the model, meaning they have low bias and high variance. Random Forest produces models less affected by outlyers, they will have a higher bias than Bagged Decision Trees, but the variance will be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "-----------------------------\n",
      " \n",
      "LR RMSE Train Score is 20.25386912005579\n",
      "LR RMSE Test Score is 21.638157117706292\n",
      " \n",
      "KNN RMSE Train Score is 16.50221997028363\n",
      "KNN RMSE Test Score is 20.362884240446483\n",
      " \n",
      "DT RMSE Train Score is 2.066312589899166\n",
      "DT RMSE Test Score is 27.52552798980154\n",
      " \n",
      "Bagged RMSE Train Score is 8.859451747126272\n",
      "Bagged RMSE Test Score is 21.100026829043387\n",
      " \n",
      "RF RMSE Train Score is 7.633770631503993\n",
      "RF RMSE Test Score is 20.39824502403272\n",
      " \n",
      "Adaboost RMSE Train Score is 21.956798373463457\n",
      "Adaboost RMSE Test Score is 23.112219825463253\n",
      " \n",
      "SVC RMSE Train Score is 19.76896191032628\n",
      "SVC RMSE Test Score is 20.385095627961324\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "obj.RMSE_Calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are overfit:    \n",
    "- Decision Tree\n",
    "- Bagging\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I would use the SVC, but Adaboost or a regular old LinearRegression would also work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** As always, more data would be nice to have. Each of these methods could be gridsearched over for the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer:** We are trying to predict if someone is eligible for a 401k. If they are enrolled in a 401K then they are obviously eligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the below are appropriate:  \n",
    "- logistic regression \n",
    "- k-nearest neighbors\n",
    "- decision tree\n",
    "- bagged decision trees\n",
    "- random forest\n",
    "- Adaboost model\n",
    "- support vector classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['marr', 'male', 'age', 'fsize', 'nettfa', 'agesq', 'inc', 'incsq', 'pira']\n",
    "obj = analyzer(df, features, 'e401k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "-----------------------------\n",
      " \n",
      "LG F1 Train Score is 0.48493277700509974\n",
      "LG F1 Test Score is 0.471169686985173\n",
      " \n",
      "KNN F1 Train Score is 0.6611779607346423\n",
      "KNN F1 Test Score is 0.48469643753135977\n",
      " \n",
      "DT F1 Train Score is 1.0\n",
      "DT F1 Test Score is 0.4688950789229341\n",
      " \n",
      "Bagged F1 Train Score is 0.9693104822638502\n",
      "Bagged F1 Test Score is 0.49376299376299376\n",
      " \n",
      "RF F1 Train Score is 1.0\n",
      "RF F1 Test Score is 0.5219586067642605\n",
      " \n",
      "Adaboost F1 Train Score is 0.5742279020234291\n",
      "Adaboost F1 Test Score is 0.5538150581101566\n",
      " \n",
      "SV F1 Train Score is 0.4769853313100658\n",
      "SV F1 Test Score is 0.45542168674698796\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "obj.go_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "False positive: Predicting someone is eligible but they are not   \n",
    "False negative: Predicting someone is not eligible but they are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Definately minimize false negatives. You want as much business as possible, so people showing up but then not being eligible is better than people not showing up at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** F1 score is an average of sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "-----------------------------\n",
      " \n",
      "LG F1 Train Score is 0.48493277700509974\n",
      "LG F1 Test Score is 0.471169686985173\n",
      " \n",
      "KNN F1 Train Score is 0.6611779607346423\n",
      "KNN F1 Test Score is 0.48469643753135977\n",
      " \n",
      "DT F1 Train Score is 1.0\n",
      "DT F1 Test Score is 0.47708138447146864\n",
      " \n",
      "Bagged F1 Train Score is 0.9704472843450479\n",
      "Bagged F1 Test Score is 0.4726704841228527\n",
      " \n",
      "RF F1 Train Score is 1.0\n",
      "RF F1 Test Score is 0.5276381909547738\n",
      " \n",
      "Adaboost F1 Train Score is 0.5742279020234291\n",
      "Adaboost F1 Test Score is 0.5538150581101566\n",
      " \n",
      "SV F1 Train Score is 0.4769853313100658\n",
      "SV F1 Test Score is 0.45542168674698796\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# I already used this metric above, but just for kicks here it is again:\n",
    "obj.go_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Answer:** \n",
    "The following are overfit:\n",
    "- decision tree\n",
    "- bagged\n",
    "- random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Adaboost has the best scores. I would use that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I would gridsearch over the Adaboost parameters to find the best model. Also more data, employment status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
